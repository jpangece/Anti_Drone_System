{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9heVzwpO26L-",
        "outputId": "8864b63a-fc90-4e14-9a72-cd2c3975fdd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.5/258.5 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mNot logged in\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: "
          ]
        }
      ],
      "source": [
        "!pip install transformers -q\n",
        "!pip install datasets -q\n",
        "!pip install accelerate -U -q\n",
        "!pip install wandb -q\n",
        "!huggingface-cli whoami\n",
        "!wandb login\n",
        "!huggingface-cli login\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import set_seed\n",
        "import torch\n",
        "import numpy as np\n",
        "import wandb\n",
        "\n",
        "seed = 42\n",
        "set_seed(seed)"
      ],
      "metadata": {
        "id": "lsmQTsOW3Ohj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "full_dataset = load_dataset(\"Goorm-AI-04/Drone_Doppler\")\n",
        "test_dataset = full_dataset[\"test\"]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_dataset, eval_dataset = train_test_split(full_dataset[\"train\"], test_size=0.1, stratify=full_dataset[\"train\"][\"label\"])\n",
        "\n",
        "from datasets import Dataset\n",
        "train_dataset = Dataset.from_dict(train_dataset)\n",
        "eval_dataset = Dataset.from_dict(eval_dataset)\n",
        "\n",
        "class_set = set(train_dataset[\"type\"])\n",
        "id2label = {id:label for id, label in enumerate(class_set)}\n",
        "label2id = {label:id for id, label in id2label.items()}"
      ],
      "metadata": {
        "id": "7Cc23MJL3P2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
        "\n",
        "def compute_metrics(pred):\n",
        "  labels = pred.label_ids\n",
        "  preds = pred.predictions.argmax(-1)\n",
        "  precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='micro')\n",
        "  acc = accuracy_score(labels, preds)\n",
        "  return {\n",
        "      'accuracy': acc,\n",
        "      'f1': f1,\n",
        "      'precision': precision,\n",
        "      'recall': recall,\n",
        "  }\n",
        "\n",
        "def ceiling(array, ceiling = 1):\n",
        "  array[array>ceiling] = ceiling\n",
        "  return array\n",
        "\n",
        "def floor(array, floor = 0):\n",
        "  array[array<floor] = floor\n",
        "  return array\n",
        "\n",
        "def collate_fn(examples):\n",
        "  from torchvision import transforms\n",
        "  from PIL import Image\n",
        "  normalize = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "  ])\n",
        "  concat = lambda x : np.concatenate([x,x,x], axis=2)\n",
        "  pixel_values = torch.tensor(np.array([np.array(normalize(concat(np.expand_dims(np.array(Image.fromarray(np.array(example[\"image\"])).resize((224,224))),axis=2)))) for example in examples])).float()\n",
        "  labels = torch.tensor([example[\"label\"] for example in examples])\n",
        "  return {\"x\": pixel_values, \"labels\": labels}"
      ],
      "metadata": {
        "id": "CiEA4v1X3RGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)"
      ],
      "metadata": {
        "id": "KO7cEo2-3T-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedModel, PretrainedConfig\n",
        "\n",
        "class GoogLeNetConfig(PretrainedConfig):\n",
        "  def __init__(self,**kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "class GoogLeNet(PreTrainedModel):\n",
        "  def __init__(self, model, config):\n",
        "    super().__init__(config)\n",
        "    self.model = model\n",
        "    self.cross_entropy = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "  def forward(self, x, labels):\n",
        "    logits = self.model(x)\n",
        "    if labels is not None:\n",
        "      loss = self.cross_entropy(logits, labels)\n",
        "      return {\"loss\": loss, \"logits\":logits}\n",
        "    return {\"logits\":logits}"
      ],
      "metadata": {
        "id": "LF3dY94v3S4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run(seed):\n",
        "  if wandb.run is not None:\n",
        "    wandb.finish()\n",
        "\n",
        "  set_seed(seed)\n",
        "\n",
        "  import torch\n",
        "  model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)\n",
        "\n",
        "  import torch.nn as nn\n",
        "  model.fc = nn.Linear(1024,3)\n",
        "\n",
        "  config = GoogLeNetConfig()\n",
        "  model = GoogLeNet(model, config)\n",
        "\n",
        "  from transformers import TrainingArguments\n",
        "  training_args = TrainingArguments(\n",
        "      output_dir='./drive/MyDrive/FMCW/AlexNet/results',          # output directory\n",
        "      num_train_epochs=12,              # total number of training epochs\n",
        "      learning_rate=1e-3,\n",
        "      per_device_train_batch_size=128,   # batch size per device during training\n",
        "      per_device_eval_batch_size=20,   # batch size for evaluation\n",
        "      warmup_steps=16,               # number of warmup steps for learning rate scheduler\n",
        "      weight_decay=0.001,               # strength of weight decay\n",
        "      logging_dir='./drive/MyDrive/FMCW/AlexNet/logs',            # directory for storing logs\n",
        "      logging_steps=4,               # How often to print logs\n",
        "      do_train=True,                   # Perform training\n",
        "      do_eval=True,                    # Perform evaluation\n",
        "      evaluation_strategy=\"epoch\",     # evalute after eachh epoch\n",
        "      gradient_accumulation_steps=1,  # total number of steps before back propagation\n",
        "      fp16=True,                       # Use mixed precision\n",
        "      run_name=\"FMCW_AlexNet\",       # experiment name\n",
        "      seed=seed,                           # Seed for experiment reproducibility\n",
        "      remove_unused_columns=False,\n",
        "      report_to=\"wandb\",\n",
        "      # load_best_model_at_end=True,\n",
        "      # metric_for_best_model=metric_name,\n",
        "  )\n",
        "\n",
        "  from datetime import datetime\n",
        "  wandb.init(\n",
        "      # set the wandb project where this run will be logged\n",
        "      project=f\"FMCW_AlexNet\",\n",
        "      name=f\"{datetime.now().strftime('%b-%d %H:%M')} lr:{training_args.learning_rate:1.0e} batch_size:{training_args.per_device_train_batch_size} epoch:{training_args.num_train_epochs}\",\n",
        "\n",
        "      # track hyperparameters and run metadata\n",
        "      config=training_args\n",
        "  )\n",
        "\n",
        "  from transformers import Trainer\n",
        "\n",
        "  trainer = Trainer(\n",
        "      model=model,\n",
        "      args=training_args,\n",
        "      train_dataset=train_dataset,\n",
        "      eval_dataset=eval_dataset,\n",
        "      compute_metrics=compute_metrics,\n",
        "      data_collator=collate_fn\n",
        "  )\n",
        "\n",
        "  trainer.train()\n",
        "\n",
        "  return trainer, model\n",
        "\n",
        "end_trainer, end_model = run(seed)"
      ],
      "metadata": {
        "id": "1KTrWVbB3WcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "75hpsNeq3YGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "end_trainer.predict(test_dataset).metrics"
      ],
      "metadata": {
        "id": "a5LCHR0i3aBK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}